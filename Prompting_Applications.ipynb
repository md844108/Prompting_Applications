{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcb0781",
   "metadata": {},
   "source": [
    "# PAL (Program-Aided Language Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff87459",
   "metadata": {},
   "source": [
    "Gao et al., (2022)(opens in a new tab) presents a method that uses LLMs to read natural language problems and generate programs as the intermediate reasoning steps. Coined, program-aided language models (PAL), it differs from chain-of-thought prompting in that instead of using free-form text to obtain solution it offloads the solution step to a programmatic runtime such as a Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a791907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0717ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c05dd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99b18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first configure a few things:\n",
    "load_dotenv()\n",
    " \n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"Enter-api-key\")\n",
    " \n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Enter-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33edc04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irfan\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:179: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "C:\\Users\\irfan\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:751: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup model instance:\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f147a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup prompt + question:\n",
    "question = \"Today is 27 February 2023. I was born exactly 25 years ago. What is the date I was born in MM/DD/YYYY?\"\n",
    " \n",
    "DATE_UNDERSTANDING_PROMPT = f\"\"\"\n",
    "# Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n",
    "# If 2015 is coming in 36 hours, then today is 36 hours before.\n",
    "today = datetime(2015, 1, 1) - relativedelta(hours=36)\n",
    "# One week from today,\n",
    "one_week_from_today = today + relativedelta(weeks=1)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "one_week_from_today.strftime('%m/%d/%Y')\n",
    "# Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY?\n",
    "# If the first day of 2019 is a Tuesday, and today is the first Monday of 2019, then today is 6 days later.\n",
    "today = datetime(2019, 1, 1) + relativedelta(days=6)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "today.strftime('%m/%d/%Y')\n",
    "# Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?\n",
    "# If the concert was scheduled to be on 06/01/1943, but was delayed by one day to today, then today is one day later.\n",
    "today = datetime(1943, 6, 1) + relativedelta(days=1)\n",
    "# 10 days ago,\n",
    "ten_days_ago = today - relativedelta(days=10)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "ten_days_ago.strftime('%m/%d/%Y')\n",
    "# Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?\n",
    "# It is 4/19/1969 today.\n",
    "today = datetime(1969, 4, 19)\n",
    "# 24 hours later,\n",
    "later = today + relativedelta(hours=24)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "today.strftime('%m/%d/%Y')\n",
    "# Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?\n",
    "# If Jane thought today is 3/11/2002, but today is in fact Mar 12, then today is 3/1/2002.\n",
    "today = datetime(2002, 3, 12)\n",
    "# 24 hours later,\n",
    "later = today + relativedelta(hours=24)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "later.strftime('%m/%d/%Y')\n",
    "# Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY?\n",
    "# If Jane was born on the last day of Feburary in 2001 and today is her 16-year-old birthday, then today is 16 years later.\n",
    "today = datetime(2001, 2, 28) + relativedelta(years=16)\n",
    "# Yesterday,\n",
    "yesterday = today - relativedelta(days=1)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "yesterday.strftime('%m/%d/%Y')\n",
    "# Q: {question}\n",
    "\"\"\".strip() + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a15332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# If today is 27 February 2023 and I was born exactly 25 years ago, then I was born 25 years before.\n",
      "today = datetime(2023, 2, 27)\n",
      "# I was born 25 years before,\n",
      "born = today - relativedelta(years=25)\n",
      "# The answer formatted with %m/%d/%Y is\n",
      "born.strftime('%m/%d/%Y')\n"
     ]
    }
   ],
   "source": [
    "llm_out = llm(DATE_UNDERSTANDING_PROMPT.format(question=question))\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78d7204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998-02-27 00:00:00\n"
     ]
    }
   ],
   "source": [
    "exec(llm_out)\n",
    "print(born)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291acf9c",
   "metadata": {},
   "source": [
    "# Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a9127",
   "metadata": {},
   "source": [
    "LLMs have strong capabilities to generate coherent text. Using effective prompt strategies can steer the model to produce better, consistent, and more factual responses. LLMs can also be especially useful for generating data which is really useful to run all sorts of experiments and evaluations. For example, we can use it to generate quick samples for a sentiment classifier like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e81e4592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Negative Examples:\n",
      "Q: I'm so disappointed with the customer service I received.\n",
      "A: Negative\n",
      "\n",
      "Q: I can't believe how terrible this product is.\n",
      "A: Negative\n",
      "\n",
      "Positive Examples:\n",
      "Q: I'm so pleased with the quality of this product.\n",
      "A: Positive\n",
      "\n",
      "Q: The customer service I received was outstanding.\n",
      "A: Positive\n",
      "\n",
      "Q: I'm so happy with my purchase.\n",
      "A: Positive\n",
      "\n",
      "Q: I'm really impressed with the speed of delivery.\n",
      "A: Positive\n",
      "\n",
      "Q: I'm so satisfied with the results.\n",
      "A: Positive\n",
      "\n",
      "Q: I'm really pleased with the value for money.\n",
      "A: Positive\n",
      "\n",
      "Q: I'm really impressed with the level of service.\n",
      "A: Positive\n",
      "\n",
      "Q: I'm so delighted with the outcome.\n",
      "A: Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Produce 10 exemplars for sentiment analysis. Examples are categorized as either positive or negative. Produce 2 negative examples and 8 positive examples. Use this format for the examples:\n",
    "Q: <sentence>\n",
    "A: <sentiment>\"\"\"\n",
    "\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf50d3",
   "metadata": {},
   "source": [
    "# Generating Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e615cff",
   "metadata": {},
   "source": [
    "LLMs like ChatGPT are very effective at code generation. In this section, we will cover many examples of how to use ChatGPT for code generation.\n",
    "\n",
    "The OpenAI's Playground (Chat Mode) and the gpt-3.5-turbo model are used for all examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf378650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = input(\"What is your name? \")\n",
      "print(\"Hello\", name)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write code in python language that asks the user for their name and say 'Hello'\"\"\"\n",
    "\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "636b603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies = [\"The Shawshank Redemption\", \"The Godfather\", \"The Dark Knight\", \"The Godfather: Part II\", \"12 Angry Men\", \"Schindler's List\", \"The Lord of the Rings: The Return of the King\", \"Pulp Fiction\", \"The Good, the Bad and the Ugly\", \"Fight Club\"]\n",
      "ratings = [9.3, 9.2, 9.0, 9.0, 8.9, 8.9, 8.9, 8.9, 8.8, 8.8]\n",
      "\n",
      "movie_ratings = {}\n",
      "\n",
      "for i in range(len(movies)):\n",
      "    movie_ratings[movies[i]] = ratings[i]\n",
      "\n",
      "json_object = json.dumps(movie_ratings, indent=4)\n",
      "\n",
      "print(json_object)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write code in python language \n",
    "1. Create a list of movies\n",
    "2. Create a list of ratings for these movies\n",
    "3. Combine them to make a json object of 10 movies with their ratings.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986e5f7",
   "metadata": {},
   "source": [
    "## Complete Functions or Next Line\n",
    "These LLMs have also been incorporated into tools like GitHub Copilot which makes them useful for developers. One useful feature is the ability of the model to complete functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73a453ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply(num1, num2):\n",
      "    result = (num1 * num2) + 75\n",
      "    return result\n",
      "\n",
      "# example usage\n",
      "print(multiply(5, 10)) # output: 125\n",
      "print(multiply(2, 3)) # output: 81\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write code in python language\n",
    "# function to multiply two numbers and add 75 to it\n",
    "def multiply(\n",
    "\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf3706",
   "metadata": {},
   "source": [
    "## MySQL Query Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7f855",
   "metadata": {},
   "source": [
    "Besides the basic code generation example above, you can use the model to generate useful code that could be used in other aspects of programming like creating and testing MySQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b157e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT StudentId, StudentName \n",
      "FROM students \n",
      "WHERE DepartmentId = (SELECT DepartmentId FROM departments WHERE DepartmentName = 'Computer Science')\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Table departments, columns = [DepartmentId, DepartmentName]\n",
    "Table students, columns = [DepartmentId, StudentId, StudentName]\n",
    "Create a MySQL query for all students in the Computer Science department\n",
    "\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9091869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE departments (\n",
      "  DepartmentId INT PRIMARY KEY,\n",
      "  DepartmentName VARCHAR(50) NOT NULL\n",
      ");\n",
      "\n",
      "CREATE TABLE students (\n",
      "  DepartmentId INT NOT NULL,\n",
      "  StudentId INT PRIMARY KEY,\n",
      "  StudentName VARCHAR(50) NOT NULL,\n",
      "  FOREIGN KEY (DepartmentId) REFERENCES departments(DepartmentId)\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Table departments, columns = [DepartmentId, DepartmentName]\n",
    "Table students, columns = [DepartmentId, StudentId, StudentName]\n",
    "Create a valid database schema with the above tables and columns\n",
    "\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b794ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO departments (DepartmentId, DepartmentName) VALUES (1, 'Computer Science');\n",
      "INSERT INTO departments (DepartmentId, DepartmentName) VALUES (2, 'Mathematics');\n",
      "INSERT INTO departments (DepartmentId, DepartmentName) VALUES (3, 'Biology');\n",
      "INSERT INTO departments (DepartmentId, DepartmentName) VALUES (4, 'History');\n",
      "\n",
      "INSERT INTO students (DepartmentId, StudentId, StudentName) VALUES (1, 1001, 'John Smith');\n",
      "INSERT INTO students (DepartmentId, StudentId, StudentName) VALUES (1, 1002, 'Jane Doe');\n",
      "INSERT INTO students (DepartmentId, StudentId, StudentName) VALUES (2, 1003, 'Bob Johnson');\n",
      "INSERT INTO students (DepartmentId, StudentId, StudentName) VALUES (3, 1004, 'Samantha Lee');\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "CREATE TABLE departments (\n",
    "  DepartmentId INT PRIMARY KEY,\n",
    "  DepartmentName VARCHAR(50)\n",
    ");\n",
    "CREATE TABLE students (\n",
    "  DepartmentId INT,\n",
    "  StudentId INT PRIMARY KEY,\n",
    "  StudentName VARCHAR(50),\n",
    "  FOREIGN KEY (DepartmentId) REFERENCES departments(DepartmentId)\n",
    ");\n",
    "Given the database schema above, generate valid insert statements include 4 rows for each table.\n",
    "\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226cfd65",
   "metadata": {},
   "source": [
    "## Explain Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9661dc",
   "metadata": {},
   "source": [
    "If you are learning to program in a certain language, it might be useful to prompt the model to explain certain bits of code. Let's reuse the query generated above and ask the model to explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1661a0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SQL statement selects the StudentId and StudentName columns from the students table. It then performs an inner join with the departments table on the DepartmentId column of both tables. The WHERE clause filters the results to only include students who belong to the department with the name 'Computer Science'. The result is a list of all students who are enrolled in the Computer Science department.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "SELECT students.StudentId, students.StudentName\n",
    "FROM students\n",
    "INNER JOIN departments\n",
    "ON students.DepartmentId = departments.DepartmentId\n",
    "WHERE departments.DepartmentName = 'Computer Science';\n",
    "Explain the above SQL statement.\n",
    "\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155488a",
   "metadata": {},
   "source": [
    "# Graduate Job Classification Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d1231",
   "metadata": {},
   "source": [
    "https://www.promptingguide.ai/applications/workplace_casestudy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a349a",
   "metadata": {},
   "source": [
    "# Prompt Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ef0d1",
   "metadata": {},
   "source": [
    "When we draw a parallel between GPT's dialogue interface and a programming language's shell, the encapsulation prompt can be thought of as forming a function. This function has a unique name, and when we call this name with the input text, it produces results based on the set internal rules. In a nutshell, we build a reusable prompt with a name that makes it easy to engage with GPT. It's like having a handy tool that lets GPT carry out particular tasks on our behalf – we just need to give the input, and we receive the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0b4d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Hello, ChatGPT! I hope you are doing well. I am reaching out to you for assistance with a specific function. I understand that you have the capability to process information and perform various tasks based on the instructions provided. In order to help you understand my request more easily, I will be using a template to describe the function, input, and instructions on what to do with the input. Please find the details below:\n",
    "function_name: [Function Name]\n",
    "input: [Input]\n",
    "rule: [Instructions on how to process the input]\n",
    "I kindly request you to provide the output for this function, based on the details I have provided. Your assistance is greatly appreciated. Thank you!\n",
    "I will replace the text inside the brackets with the relevant information for the function I want you to perform. This detailed introduction should help you understand my request more efficiently and provide the desired output. The format is function_name(input) If you understand, just answer one word with ok.\n",
    "\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53e56a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [\"translated_text\"]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "function_name: [trans_word]\n",
    "input: [\"text\"]\n",
    "rule: [I want you to act as an English translator, spelling corrector and improver. I will provide you with input forms including \"text\" in any language and you will detect the language, translate it and answer in the corrected of my text, in English.]\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a748930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def expand_word(text):\n",
      "    \"\"\"\n",
      "    This function takes in a text and expands the words to make it more literary while keeping the meaning the same.\n",
      "    \"\"\"\n",
      "    # Import necessary libraries\n",
      "    import nltk\n",
      "    from nltk.corpus import wordnet\n",
      "    \n",
      "    # Tokenize the text\n",
      "    tokens = nltk.word_tokenize(text)\n",
      "    \n",
      "    # Create an empty list to store the expanded words\n",
      "    expanded_words = []\n",
      "    \n",
      "    # Loop through each token\n",
      "    for token in tokens:\n",
      "        # Get the synonyms of the token\n",
      "        synonyms = wordnet.synsets(token)\n",
      "        \n",
      "        # If there are synonyms, get the first one and add it to the expanded words list\n",
      "        if synonyms:\n",
      "            expanded_words.append(synonyms[0].lemmas()[0].name())\n",
      "        # If there are no synonyms, add the original token to the expanded words list\n",
      "        else:\n",
      "            expanded_words.append(token)\n",
      "    \n",
      "    # Join the expanded words list back into a string\n",
      "    expanded_text = ' '.join(expanded_words)\n",
      "    \n",
      "    # Return the expanded text\n",
      "    return expanded_text\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "    \n",
    "prompt = \"\"\"\n",
    "function_name: [expand_word]\n",
    "input: [\"text\"]\n",
    "rule: [Please serve as a Chatterbox, spelling corrector, and language enhancer. I will provide you with input forms including \"text\" in any language, and output the original language.I want you to Keep the meaning same, but make them more literary.]\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d30e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fix_english(text):\n",
      "    # import necessary libraries\n",
      "    import nltk\n",
      "    from nltk.corpus import wordnet\n",
      "    from nltk.tokenize import word_tokenize\n",
      "    from nltk.stem import WordNetLemmatizer\n",
      "    \n",
      "    # initialize lemmatizer\n",
      "    lemmatizer = WordNetLemmatizer()\n",
      "    \n",
      "    # tokenize the text\n",
      "    tokens = word_tokenize(text)\n",
      "    \n",
      "    # lemmatize the tokens\n",
      "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
      "    \n",
      "    # join the lemmatized tokens to form a sentence\n",
      "    sentence = ' '.join(lemmatized_tokens)\n",
      "    \n",
      "    # return the fixed sentence\n",
      "    return sentence\n",
      "\n",
      "# helper function to get the wordnet part of speech\n",
      "def get_wordnet_pos(word):\n",
      "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
      "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
      "    tag_dict = {\"J\": wordnet.ADJ,\n",
      "                \"N\": wordnet.NOUN,\n",
      "                \"V\": wordnet.VERB,\n",
      "                \"R\": wordnet.ADV}\n",
      "    return tag_dict.get(tag, wordnet.NOUN)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "function_name: [fix_english]\n",
    "input: [\"text\"]\n",
    "rule: [Please serve as an English master, spelling corrector, and language enhancer. I will provide you with input forms including \"text\", I want you to improve the text's vocabulary and sentences with more natural and elegent. Keep the meaning same.]\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39c7b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mount Bromo volcano is located in Indonesia, which is known as the \"Thousand Islands\" country. Indonesia, a country with many islands, has 4,500 volcanoes, and three of the world's top ten active volcanoes are located here.\n",
      "\n",
      "Finally, you can run the function independently or chain them together.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "trans_word('婆罗摩火山处于享有“千岛之国”美称的印度尼西亚. 多岛之国印尼有4500座之多的火山, 世界著名的十大活火山有三座在这里.')\n",
    "fix_english('Finally, you can run the function independently or chain them together.')\n",
    "fix_english(expand_word(trans_word('婆罗摩火山处于享有“千岛之国”美称的印度尼西亚. 多岛之国印尼有4500座之多的火山, 世界著名的十大活火山有三座在这里.')))\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d56458",
   "metadata": {},
   "source": [
    "## Multiple params function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed07ba",
   "metadata": {},
   "source": [
    "Let's create a function that generates a password by taking five input parameters, and outputs the generated password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e02492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, as an AI language model, I am not capable of generating outputs that require randomization or probability.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "function_name: [pg]\n",
    "input: [\"length\", \"capitalized\", \"lowercase\", \"numbers\", \"special\"]\n",
    "rule: [I want you to act as a password generator for individuals in need of a secure password. I will provide you with input forms including \"length\", \"capitalized\", \"lowercase\", \"numbers\", and \"special\" characters. Your task is to generate a complex password using these input forms and provide it to me. Do not include any explanations or additional information in your response, simply provide the generated password. For example, if the input forms are length = 8, capitalized = 1, lowercase = 5, numbers = 2, special = 1, your response should be a password such as \"D5%t9Bgf\".]\n",
    "\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "585973b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A possible output for the function pg() with the given parameters could be:\n",
      "\n",
      "Kl4h#jg7tL\n",
      "\n",
      "This password has a length of 10 characters, with 1 capitalized letter (K), 5 lowercase letters (l4hjg), 2 numbers (7 and 4), and 1 special character (#). Note that the order and specific characters used may vary each time the function is called.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "pg(length = 10, capitalized = 1, lowercase = 5, numbers = 2, special = 1)\n",
    "pg(10,1,5,2,1)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm_out = llm(prompt)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc3333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691d33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
